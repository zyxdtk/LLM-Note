<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>Inference - 大模型笔记</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Inference";
        var mkdocs_page_input_path = "3.[\u57fa\u5efa]\u6548\u7387/Inference.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../.." class="icon icon-home"> 大模型笔记
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../..">Home</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">0.inbox</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../0.inbox/%5BRead%5D2025-04/">[Read]2025 04</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../0.inbox/alg-interview-faq/">Alg interview faq</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">1.[基建]数据</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../1.%5B%E5%9F%BA%E5%BB%BA%5D%E6%95%B0%E6%8D%AE/data/">Data</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">3.[基建]效率</span></p>
              <ul class="current">
                  <li class="toctree-l1 current"><a class="reference internal current" href="#">Inference</a>
    <ul class="current">
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../train/">Train</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">4.[模型]文本</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../4.%5B%E6%A8%A1%E5%9E%8B%5D%E6%96%87%E6%9C%AC/Embedding/">Embedding</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../4.%5B%E6%A8%A1%E5%9E%8B%5D%E6%96%87%E6%9C%AC/PostTraining/">PostTraining</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../4.%5B%E6%A8%A1%E5%9E%8B%5D%E6%96%87%E6%9C%AC/PreTraining/">PreTraining</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">5.[模型]多模态</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../5.%5B%E6%A8%A1%E5%9E%8B%5D%E5%A4%9A%E6%A8%A1%E6%80%81/MultiModalEmbedding/">MultiModalEmbedding</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../5.%5B%E6%A8%A1%E5%9E%8B%5D%E5%A4%9A%E6%A8%A1%E6%80%81/T2V/">T2V</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../5.%5B%E6%A8%A1%E5%9E%8B%5D%E5%A4%9A%E6%A8%A1%E6%80%81/VLA/">VLA</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../5.%5B%E6%A8%A1%E5%9E%8B%5D%E5%A4%9A%E6%A8%A1%E6%80%81/VLM/">VLM</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">6.[模型]评测</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../6.%5B%E6%A8%A1%E5%9E%8B%5D%E8%AF%84%E6%B5%8B/Benchmark/">Benchmark</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">7.[应用]产品</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../7.%5B%E5%BA%94%E7%94%A8%5D%E4%BA%A7%E5%93%81/Agent/">Agent</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../7.%5B%E5%BA%94%E7%94%A8%5D%E4%BA%A7%E5%93%81/Product/">Product</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../7.%5B%E5%BA%94%E7%94%A8%5D%E4%BA%A7%E5%93%81/VibeCoding/">VibeCoding</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">大模型笔记</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">3.[基建]效率</li>
      <li class="breadcrumb-item active">Inference</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h2 id="1">1. 大模型推理学习资料<a class="headerlink" href="#1" title="Permanent link">#</a></h2>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/655557420">LLM推理优化技术详解</a> 推理加速方法</li>
<li><a href="https://github.com/vllm-project/vllm">github:vLLM</a> 高性能推理框架</li>
<li><a href="https://github.com/NVIDIA/FasterTransformer">NVIDIA/FasterTransformer</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/624740065">分析transformer模型的参数量、计算量、中间激活、KV cache</a></li>
</ul>
<h2 id="2">2. 大模型推理优化<a class="headerlink" href="#2" title="Permanent link">#</a></h2>
<p>大模型推理关注：延迟、吞吐和成本，优化分：改模型参数、单机优化、分布式优化。</p>
<ul>
<li>改模型参数。<ul>
<li>量化</li>
<li>attention结构(mha、mqa、mla、sparse attention、 liner attention)</li>
<li>ffn结构(moe)</li>
<li>其他结构(silu、rmsnorm)</li>
<li>随机解码。</li>
</ul>
</li>
<li>单机优化。LLM是io约束的。<ul>
<li>算子融合。qkv融合，bias融合。</li>
<li>高性能算子。flash attention、高性能矩阵运算gemm。需要深入到kernel层面。</li>
<li>内存管理。continuous batching、paged attention。</li>
</ul>
</li>
<li>分布式优化。<ul>
<li>模型并行。tensor并行、pipeline并行、专家并行</li>
<li>数据并行。zero3</li>
<li>硬件特化。prefill和generate分离。</li>
</ul>
</li>
</ul>
<h3 id="21">2.1. 改模型参数<a class="headerlink" href="#21" title="Permanent link">#</a></h3>
<h4 id="211">2.1.1. 量化<a class="headerlink" href="#211" title="Permanent link">#</a></h4>
<ul>
<li>[2025.01] <a href="https://arxiv.org/abs/2501.13331">Qrazor: Reliable and Effortless 4-bit LLM Quantization by Significant Data Razoring</a></li>
<li>[2023.10] <a href="https://arxiv.org/abs/2310.16836">LLM-FP4: 4-Bit Floating-Point Quantized Transformers</a></li>
<li>[2023.09] <a href="https://arxiv.org/abs/2309.05516">Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs</a> AutoRound<ul>
<li><a href="https://github.com/intel/auto-round">intel/auto-round</a></li>
</ul>
</li>
<li>[2023.06] <a href="https://arxiv.org/abs/2306.00978">AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration</a> AWQ</li>
<li>[2022.10] <a href="https://arxiv.org/abs/2210.17323">GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers</a> GPTQ</li>
<li>[2022.08] <a href="https://arxiv.org/abs/2208.07339">LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale</a><ul>
<li><a href="https://arxiv.org/abs/2208.07339">LLM.int8(): 8-bit量化推理</a></li>
</ul>
</li>
</ul>
<h4 id="212_attention">2.1.2. attention结构<a class="headerlink" href="#212_attention" title="Permanent link">#</a></h4>
<ul>
<li><a href="https://github.com/deepseek-ai/FlashMLA">FlashMLA</a> </li>
</ul>
<h4 id="213">2.1.3. 并行解码<a class="headerlink" href="#213" title="Permanent link">#</a></h4>
<ul>
<li>[2024.01] <a href="https://arxiv.org/abs/2401.15077">EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty</a><ul>
<li><a href="https://zhuanlan.zhihu.com/p/15955544919">论文解读】EAGLE：在特征层进行自回归的投机采样框架</a></li>
</ul>
</li>
<li>[2024.01] <a href="https://arxiv.org/abs/2401.10774">Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads</a> 加多个解码头，用topk解码多个token，用tree attention判定是否采纳。</li>
<li>[2023.10] <a href="https://arxiv.org/abs/2312.12728">Lookahead: An Inference Acceleration Framework for Large Language Model with Lossless Generation Accuracy</a> 用2D窗口维护多个ngram</li>
<li>[2022.11] <a href="https://arxiv.org/abs/2211.17192">Fast Inference from Transformers via Speculative Decoding</a> 小模型预估，大模型判定是否采纳。计算量不变，但是可以并行化了。</li>
</ul>
<h3 id="22">2.2. 单机优化<a class="headerlink" href="#22" title="Permanent link">#</a></h3>
<h4 id="221_attention">2.2.1. attention<a class="headerlink" href="#221_attention" title="Permanent link">#</a></h4>
<ul>
<li>[2023.09] <a href="https://arxiv.org/abs/2309.06180">Efficient Memory Management for Large Language Model Serving with PagedAttention</a> PagedAttention,虚拟内存技术，分页。比朴素batch快22倍吞吐，比ft快4倍。<ul>
<li><a href="https://github.com/vllm-project/vllm">vllm-project/vllm</a></li>
<li><a href="https://www.anyscale.com/blog/continuous-batching-llm-inference">How continuous batching enables 23x throughput in LLM inference while reducing p50 latency</a></li>
</ul>
</li>
<li>[2023.08] <a href="https://arxiv.org/abs/2308.16369">SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills
</a> Chunk Prefills <ul>
<li><a href="https://zhuanlan.zhihu.com/p/14689463165">LLM推理优化 - Chunked prefills</a></li>
</ul>
</li>
<li>[2022.05] <a href="https://arxiv.org/abs/2205.14135">FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</a> FlashAttention，提速2.4倍</li>
</ul>
<h4 id="222_ffn">2.2.2. FFN<a class="headerlink" href="#222_ffn" title="Permanent link">#</a></h4>
<ul>
<li>[2023.06] [MoE: An Efficient Mixture of Experts for Large Language Models](URL_ADDRESS- [2023.06] <a href="https://arxiv.org/abs/2306.05087">MoE: An Efficient Mixture of Experts for Large Language Models</a></li>
<li><a href="https://github.com/iVishalr/GEMM">GEMM</a> 矩阵算子优化</li>
<li><a href="https://github.com/deepseek-ai/DeepGEMM">DeepGEMM</a> FP8矩阵算子优化</li>
<li><a href="https://github.com/deepseek-ai/DeepEP">DeepEP</a> 专家并行</li>
</ul>
<h3 id="23">2.3. 分布式优化<a class="headerlink" href="#23" title="Permanent link">#</a></h3>
<ul>
<li><a href="https://github.com/deepseek-ai/3FS">3FS</a> 分布式文件系统</li>
<li>[2022.12] <a href="https://dl.acm.org/doi/abs/10.1145/3567955.3567959">Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models</a></li>
<li>[2022.11] <a href="https://arxiv.org/abs/2211.05102">Efficiently Scaling Transformer Inference</a> <ul>
<li><a href="https://zhuanlan.zhihu.com/p/660715870">大模型并行推理的太祖长拳：解读Jeff Dean署名MLSys 23杰出论文</a></li>
</ul>
</li>
<li>[2022.07] <a href="https://www.usenix.org/system/files/osdi22-zheng-lianmin.pdf">Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/9816504195">大模型推理序列并行</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/720387198">序列并行DeepSpeed-FPDT</a></li>
</ul>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../../1.%5B%E5%9F%BA%E5%BB%BA%5D%E6%95%B0%E6%8D%AE/data/" class="btn btn-neutral float-left" title="Data"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../train/" class="btn btn-neutral float-right" title="Train">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../../1.%5B%E5%9F%BA%E5%BB%BA%5D%E6%95%B0%E6%8D%AE/data/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../train/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../..";</script>
    <script src="../../js/theme_extra.js"></script>
    <script src="../../js/theme.js"></script>
      <script src="../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
