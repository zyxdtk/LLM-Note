


## 训练框架

- [deepspeedai/DeepSpeed](https://github.com/deepspeedai/DeepSpeed)
- [unslothai/unsloth](https://github.com/unslothai/unsloth) Finetune框架


## 强化学习训练框架

- [54.7k] [hiyouga/LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)
- [14.7k] [huggingface/trl](https://github.com/huggingface/trl)
- [11.3k] [volcengine/verl](https://github.com/volcengine/verl) 字节
- [8.8k] [modelscope/ms-swift](https://github.com/modelscope/ms-swift) 
- [7.4k] [OpenRLHF/OpenRLHF](https://github.com/OpenRLHF/OpenRLHF) 
- [1.5k] [alibaba/ROLL](https://github.com/alibaba/ROLL)

## 分布式训练

- [2024.10] [Liger Kernel: Efficient Triton Kernels for LLM Training](https://arxiv.org/abs/2410.10989)
    - [linkedin/Liger-Kernel](https://github.com/linkedin/Liger-Kernel) 
- [2023.04] [PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel](https://arxiv.org/abs/2304.11277)
- [2019.10] [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/abs/1910.02054)
    - [[LLM]大模型显存计算公式与优化](https://zhuanlan.zhihu.com/p/687226668)

