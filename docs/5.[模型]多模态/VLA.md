# VLA

## 开源工作

- [2025.06] [AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning](https://arxiv.org/abs/2506.13757)
- [2025.06] [Unified Vision-Language-Action Model](https://www.arxiv.org/abs/2506.19850)
- [2025.05] [Vision-Language-Action Models: Concepts, Progress, Applications and Challenges](https://arxiv.org/abs/2505.04769)
- [2025.03] [CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models](https://arxiv.org/abs/2503.22020)
    - [https://cot-vla.github.io/](https://cot-vla.github.io/)
- [2024.06] [OpenVLA: An Open-Source Vision-Language-Action Model](https://arxiv.org/abs/2406.09246)
    - [https://github.com/openvla/openvla](https://github.com/openvla/openvla)
- [2024.03] [Octo: An Open-Source Generalist Robot Policy](https://arxiv.org/abs/2405.12213)
    - [https://octo-models.github.io/](https://octo-models.github.io/) 
- [2023.07] [RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control](https://arxiv.org/abs/2307.15818)
- [2023.03] [PaLM-E: An Embodied Multimodal Language Model](https://arxiv.org/abs/2303.03378)
- [2023.03] [Diffusion Policy: Visuomotor Policy Learning via Action Diffusion](https://arxiv.org/abs/2303.04137v4) 
- [2022.12] [RT-1: Robotics Transformer for Real-World Control at Scale](https://arxiv.org/abs/2212.06817)
- [2022.08] [Do As I Can, Not As I Say:Grounding Language in Robotic Affordances](https://arxiv.org/pdf/2204.01691)
    - [https://say-can.github.io/](https://say-can.github.io/)
- [2022.05] [Gato: A Generalist Agent](https://arxiv.org/abs/2205.06175)

## 数据集

- [2023.10] [Open X-Embodiment: Robotic Learning Datasets and RT-X Models](https://arxiv.org/abs/2310.08864)
    - [https://robotics-transformer-x.github.io/](https://robotics-transformer-x.github.io/)