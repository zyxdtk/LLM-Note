# 多模态大模型

## 学习资料



## 开源项目

- [2025.04] [Kimi-VL Technical Report](https://arxiv.org/abs/2504.07491)
    - [MoonshotAI/Kimi-VL](https://github.com/MoonshotAI/Kimi-VL)
- [2025.01] [MiniMax-01: Scaling Foundation Models with Lightning Attention](https://arxiv.org/abs/2501.08313)
    - [MiniMax-AI/MiniMax-01](https://github.com/MiniMax-AI/MiniMax-01)
- [2024.03] [VisionLLaMA: A Unified LLaMA Backbone for Vision Tasks](https://arxiv.org/abs/2403.00522)  RoPE-2D的引入有助于提升模型效果尤其是变分辨率输入的效果
    - [Meituan-AutoML/VisionLLaMA](https://github.com/Meituan-AutoML/VisionLLaMA)
- [minimind-v](https://github.com/jingyaogong/minimind-v) 极简vlm模型
- [LLaVA](https://github.com/haotian-liu/LLaVA)
    - [2023.10] [Improved Baselines with Visual Instruction Tuning](https://arxiv.org/abs/2310.03744)  
    - [2023.04] [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485)




### Qwen


- [2025.02] [Qwen2.5-VL Technical Report](https://arxiv.org/abs/2502.13923)
    - [QwenLM/Qwen2.5-VL](https://github.com/QwenLM/Qwen2.5-VL)
- [2023.08] [Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond](https://arxiv.org/abs/2308.12966)
    - [Qwen-VL](https://github.com/QwenLM/Qwen-VL)

### Deepseek

- [DeepSeek-VL](https://github.com/deepseek-ai/DeepSeek-VL)


## 核心模块

### Encoder-Decoder

- [2021.02] [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)
    - [openai-clip](https://openai.com/index/clip/)
    - [clip-vit-base-patch16](https://huggingface.co/openai/clip-vit-base-patch16)

### 位置编码

- [2024.09] [“闭门造车”之多模态思路浅谈（三）：位置编码](https://spaces.ac.cn/archives/10352/comment-page-1)
- [2024.03] [RoPE-Tie Transformer升级之路：17、多模态位置编码的简单思考](https://spaces.ac.cn/archives/10040)